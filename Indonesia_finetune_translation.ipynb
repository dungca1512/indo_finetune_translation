{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BKN6BaCFsCe3",
    "outputId": "c77412b7-ffa4-4167-c762-2ca26d60b843"
   },
   "outputs": [],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a7449d73",
    "outputId": "5c03b666-2e44-4121-bad5-58e9d1bd675c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "print(\"Đã cài đặt biến môi trường PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dRNNtFRCslrm",
    "outputId": "52b72d64-fd34-49ce-8239-a868726e79fe"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "\n",
    "DATA_PATH = \"/root/indo_finetune_translation/data/\"\n",
    "\n",
    "# Đường dẫn đến hai tệp ngôn ngữ\n",
    "FILE_EN = os.path.join(DATA_PATH, \"WikiMatrix.en-id.en\")\n",
    "FILE_ID = os.path.join(DATA_PATH, \"WikiMatrix.en-id.id\")\n",
    "\n",
    "# Tên model trên Hugging Face\n",
    "MODEL_NAME = \"SeaLLMs/SeaLLMs-v3-1.5B-Chat\"\n",
    "\n",
    "# Thư mục lưu kết quả (adapter)\n",
    "# /kaggle/working/ là thư mục output của Kaggle\n",
    "OUTPUT_DIR = \"/root/indo_finetune_translation/data/results_finetune\"\n",
    "\n",
    "# Số lượng mẫu huấn luyện (để tránh quá 12 giờ)\n",
    "# 500,000 là con số an toàn\n",
    "NUM_SAMPLES = 150000\n",
    "\n",
    "print(f\"Đã cấu hình xong. Sẽ tải model: {MODEL_NAME}\")\n",
    "print(f\"Sẽ tải dữ liệu từ: {DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "id": "X_-btCIDJPLo",
    "outputId": "7aee2ba4-235c-4f5a-d92d-eae63dd297b9"
   },
   "outputs": [],
   "source": [
    "print(\"Bắt đầu tải và xử lý dữ liệu...\")\n",
    "\n",
    "# 1. Tải hai tệp riêng biệt\n",
    "ds_en = load_dataset(\"text\", data_files={\"train\": FILE_EN})['train']\n",
    "ds_id = load_dataset(\"text\", data_files={\"train\": FILE_ID})['train']\n",
    "\n",
    "# Đổi tên cột\n",
    "ds_en = ds_en.rename_column(\"text\", \"en_text\")\n",
    "ds_id = ds_id.rename_column(\"text\", \"id_text\")\n",
    "\n",
    "# 2. Ghép 2 bộ dữ liệu (yêu cầu số dòng bằng nhau)\n",
    "if len(ds_en) != len(ds_id):\n",
    "    raise ValueError(f\"Hai tệp không có cùng số dòng! Tiếng Anh: {len(ds_en)}, Tiếng Indo: {len(ds_id)}\")\n",
    "\n",
    "dataset = concatenate_datasets([ds_en, ds_id], axis=1)\n",
    "print(f\"Đã ghép thành công! Tổng số cặp câu: {len(dataset)}\")\n",
    "\n",
    "# 3. Lấy mẫu (sample)\n",
    "print(f\"Đang lấy mẫu ngẫu nhiên {NUM_SAMPLES} cặp câu...\")\n",
    "dataset_sampled = dataset.shuffle(seed=42).select(range(NUM_SAMPLES))\n",
    "\n",
    "# 4. Hàm format dữ liệu sang dạng Chat\n",
    "def format_data_for_chat(example):\n",
    "    eng_text = example['en_text']\n",
    "    ind_text = example['id_text']\n",
    "\n",
    "    # Chỉ xử lý nếu cả hai câu đều có nội dung\n",
    "    if eng_text and ind_text:\n",
    "        return {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Translate the following sentence from English to Indonesian: '{eng_text}'\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": ind_text\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    else:\n",
    "        # Bỏ qua các cặp câu rỗng\n",
    "        return None\n",
    "\n",
    "# 5. Áp dụng hàm format\n",
    "print(\"Đang định dạng dữ liệu sang dạng Chat...\")\n",
    "dataset_formatted = dataset_sampled.map(\n",
    "    format_data_for_chat,\n",
    "    remove_columns=['en_text', 'id_text']\n",
    ")\n",
    "\n",
    "# 6. Tách train/validation (95% train, 5% test)\n",
    "dataset_split = dataset_formatted.train_test_split(test_size=0.05, seed=42)\n",
    "train_dataset = dataset_split['train']\n",
    "eval_dataset = dataset_split['test']\n",
    "\n",
    "print(\"\\n=== XỬ LÝ DỮ LIỆU HOÀN TẤT ===\")\n",
    "print(f\"Số lượng mẫu huấn luyện: {len(train_dataset)}\")\n",
    "print(f\"Số lượng mẫu đánh giá: {len(eval_dataset)}\")\n",
    "print(\"\\n--- Mẫu dữ liệu đã định dạng ---\")\n",
    "print(train_dataset[0]['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KL5AK-3SJRRJ"
   },
   "outputs": [],
   "source": [
    "print(f\"Đang tải model {MODEL_NAME} (chế độ 4-bit)...\")\n",
    "\n",
    "# 1. Cấu hình Quantization (QLoRA) - Chuyển sang 8-bit cho compute_dtype\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16, # Thay đổi sang float16\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# 2. Tải Model (đã được lượng tử hóa 4-bit)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16, # Explicitly set torch_dtype to float16\n",
    "    device_map=\"auto\", # Tự động phân bổ lên GPU\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.config.use_cache = False # Tắt cache khi huấn luyện\n",
    "print(\"Model đã được tải và lượng tử hóa 4-bit.\")\n",
    "\n",
    "# 3. Tải Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "# Sửa lỗi pad_token (model này không có sẵn)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenizer.padding_side = \"right\" # Quan trọng cho model CausalLM\n",
    "\n",
    "print(\"Tokenizer đã được tải và cấu hình.\")\n",
    "print(\"=== MODEL VÀ TOKENIZER SẴN SÀNG =====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "37UaFCXlJsDZ"
   },
   "outputs": [],
   "source": [
    "print(\"Đang cấu hình LoRA (PEFT) và Training Arguments...\")\n",
    "\n",
    "# 1. Cấu hình LoRA (PEFT)\n",
    "# Chúng ta chỉ định các lớp (layer) muốn \"điều hợp\"\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=32,       # Độ \"mạnh\" của adapter\n",
    "    lora_dropout=0.05,   # Tỷ lệ dropout\n",
    "    r=16,                # Rank (càng cao càng phức tạp, 16 là mức tốt)\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # (Quan trọng) Các module cần huấn luyện cho kiến trúc Qwen2\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "# 2. Cấu hình các tham số huấn luyện (Training Arguments)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,                  # Thư mục lưu kết quả\n",
    "    num_train_epochs=2,                     # 1 epoch là đủ cho SFT với dataset lớn\n",
    "    per_device_train_batch_size=4,          # Giảm batch size cho GPU T4 (giảm còn 2 nếu OOM)\n",
    "    gradient_accumulation_steps=8,          # Tăng tích lũy gradient để bù lại batch size nhỏ (2 * 8 = 16)\n",
    "    eval_steps=500,                         # Đánh giá sau mỗi 500 bước\n",
    "    save_steps=500,                         # Lưu checkpoint sau mỗi 500 bước\n",
    "    logging_steps=50,                       # Log thông tin sau mỗi 50 bước\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=True,\n",
    "    bf16=False,                              # Dùng bf16 (nhanh hơn trên GPU T4)\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"none\",                     # Tắt báo cáo lên WandB/MLflow\n",
    ")\n",
    "\n",
    "# 3. Khởi tạo SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    peft_config=peft_config,\n",
    "    args=training_args\n",
    ")\n",
    "\n",
    "print(\"=== TRAINER ĐÃ SẴN SÀNG ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AzjoDSqxRcpr"
   },
   "outputs": [],
   "source": [
    "print(\"Bắt đầu huấn luyện model...\")\n",
    "trainer.train()\n",
    "print(\"Huấn luyện hoàn tất.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "idSZnMF7aoCJ"
   },
   "outputs": [],
   "source": [
    "# Lưu model cuối cùng (chỉ là các adapter LoRA)\n",
    "final_checkpoint_dir = os.path.join(OUTPUT_DIR, \"final_checkpoint\")\n",
    "trainer.save_model(final_checkpoint_dir)\n",
    "\n",
    "print(f\"=== ĐÃ LƯU ADAPTER LoRA VÀO: {final_checkpoint_dir} ===\")\n",
    "# Bạn có thể tìm thấy thư mục này trong mục \"Output\" của Kaggle Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c_UmuJtTaotH"
   },
   "outputs": [],
   "source": [
    "print(\"Đang tải model đã fine-tune để kiểm tra...\")\n",
    "\n",
    "# 1. Dọn dẹp bộ nhớ (nếu cần)\n",
    "# del model\n",
    "# del trainer\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# 2. Tải model gốc (Base model) 4-bit\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# 3. Tải adapter LoRA đã huấn luyện và ghép vào model gốc\n",
    "# (Sử dụng đường dẫn đã lưu ở Cell 7)\n",
    "model_finetuned = PeftModel.from_pretrained(base_model, final_checkpoint_dir)\n",
    "print(\"Đã ghép adapter LoRA vào model gốc.\")\n",
    "\n",
    "# 4. (Rất nên làm) Hợp nhất (merge) để có tốc độ inference nhanh nhất\n",
    "model_finetuned = model_finetuned.merge_and_unload()\n",
    "print(\"Đã merge và unload adapter.\")\n",
    "\n",
    "# 5. Tải tokenizer (tải lại cho chắc)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 6. Kiểm tra thử\n",
    "print(\"\\n=== KIỂM TRA DỊCH THUẬT ===\")\n",
    "test_sentence = \"This new small language model is very powerful and efficient.\"\n",
    "prompt_template = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Translate the following sentence from English to Indonesian: '{test_sentence}'\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Áp dụng chat template của model\n",
    "prompt_text = tokenizer.apply_chat_template(prompt_template, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "print(f\"--- Prompt đầu vào:\\n{prompt_text}\")\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Tạo văn bản\n",
    "outputs = model_finetuned.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    temperature=0.7 # Thêm chút sáng tạo\n",
    ")\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Chỉ lấy phần trả lời của assistant\n",
    "# (Model Qwen/SeaLLM dùng <|im_start|> và <|im_end|>)\n",
    "assistant_response = response.split(\"<|im_start|>assistant\\n\")[-1]\n",
    "# Bỏ <|im_end|> nếu có\n",
    "assistant_response = assistant_response.replace(\"<|im_end|>\", \"\").strip()\n",
    "\n",
    "print(f\"\\n--- Câu gốc (EN):\\n{test_sentence}\")\n",
    "print(f\"\\n--- Câu dịch (ID):\\n{assistant_response}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "translation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
